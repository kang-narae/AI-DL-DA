{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c573fe6d",
   "metadata": {},
   "source": [
    "# 선형회귀\n",
    "\n",
    "변수 x에 의해 값이 종속적으로 변하는 변수 y.\n",
    "\n",
    "### 1) 단순 선형 회귀 분석(Simple Linear Regression Analysis)\n",
    "y= ax + b\n",
    "\n",
    "### 2) 다중 선형 회귀 분석(Multiple Linear Regression Analysis)\n",
    "\n",
    "y= ax+  bx + cx + ... d\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2de72f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 가설 세우기\n",
    " ### 예측값 구하는 함수, H(x)\n",
    "\n",
    " x와 y의 관계를 유추하기 위한 수학적 식 = 가설.\n",
    " H(x) = ax + b\n",
    " 혹은 가중치라는 의미에서\n",
    " wx + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a375c7",
   "metadata": {},
   "source": [
    "## 비용함수 : 평균제곱오차\n",
    "\n",
    "실제값과 예측값에 대한 오차에 대한 식=\n",
    "목적 함수(Objective function) ,비용 함수(Cost function) , 손실 함수(Loss function)\n",
    "\n",
    "각 문제에 적합한 비용함수가 있는데,\n",
    "회귀 문제의 경우에는 주로 평균 제곱 오차(Mean Squared Error, MSE)가 사용.\n",
    "\n",
    "\n",
    "오차 = 각 x에서의 실제값 y 와  예측값인 H(X)값의 차이.\n",
    "\n",
    "\n",
    "### 평균제곱오차 = 오차의 제곱합에 대한 평균.\n",
    "### cost(w,b) = 평균제곱오차값.\n",
    "cost(w,b)값이 최소가 되게 만드는 w와 b를 구하기."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeafe00",
   "metadata": {},
   "source": [
    "## 옵티마이저 : 경사하강법\n",
    "\n",
    "\n",
    "\n",
    "### 옵티마이저란? \n",
    "### 비용함수 cost(w,b)의 값을 최소화하는 w와 b를 찾는 최적화 알고리즘을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e367f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
